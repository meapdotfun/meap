//! Inference handling for Deepseek models
//! Provides inference configuration and response processing

use crate::error::{DeepseekError, Result};
use crate::prompt::PromptTemplate;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, RwLock};
use tracing::{debug, error, instrument, warn};
use futures::stream::{Stream, StreamExt};

/// Configuration for model inference
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceConfig {
    /// Temperature for generation (0.0-2.0)
    #[serde(default = "default_temperature")]
    pub temperature: f32,
    
    /// Top-p sampling value (0.0-1.0)
    #[serde(default = "default_top_p")]
    pub top_p: f32,
    
    /// Top-k sampling value
    #[serde(default = "default_top_k")]
    pub top_k: u32,
    
    /// Maximum tokens to generate
    #[serde(default = "default_max_tokens")]
    pub max_tokens: u32,
    
    /// Stop sequences that end generation
    #[serde(default)]
    pub stop: Vec<String>,
    
    /// Whether to stream tokens as they're generated
    #[serde(default)]
    pub stream: bool,
    
    /// Presence penalty (-2.0 to 2.0)
    #[serde(default)]
    pub presence_penalty: f32,
    
    /// Frequency penalty (-2.0 to 2.0)
    #[serde(default)]
    pub frequency_penalty: f32,
    
    /// Random seed for reproducibility
    #[serde(default)]
    pub seed: Option<u64>,
}

fn default_temperature() -> f32 { 0.7 }
fn default_top_p() -> f32 { 0.9 }
fn default_top_k() -> u32 { 40 }
fn default_max_tokens() -> u32 { 1024 }

impl Default for InferenceConfig {
    fn default() -> Self {
        Self {
            temperature: default_temperature(),
            top_p: default_top_p(),
            top_k: default_top_k(),
            max_tokens: default_max_tokens(),
            stop: Vec::new(),
            stream: false,
            presence_penalty: 0.0,
            frequency_penalty: 0.0,
            seed: None,
        }
    }
}

/// Token processing mode
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum TokenizingMode {
    /// Split incoming text into tokens
    Encode,
    /// Convert tokens back to text
    Decode,
}

/// A token generated by the model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Token {
    /// Token ID in the model's vocabulary
    pub id: u32,
    /// Text representation of the token
    pub text: String,
    /// Log probability of this token
    pub logprob: Option<f32>,
    /// Is this token a special token
    pub special: bool,
}

/// Full inference response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResponse {
    /// Generated text
    pub text: String,
    /// Tokens generated
    #[serde(default)]
    pub tokens: Vec<Token>,
    /// Usage information
    pub usage: TokenUsage,
    /// Generation metrics
    pub metrics: InferenceMetrics,
    /// Model used for generation
    pub model: String,
}

/// Token usage information
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct TokenUsage {
    /// Number of prompt tokens
    pub prompt_tokens: u32,
    /// Number of completion tokens
    pub completion_tokens: u32,
    /// Total tokens used
    pub total_tokens: u32,
}

/// Metrics about the inference process
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct InferenceMetrics {
    /// Time taken to process the request
    pub total_time: Duration,
    /// Time spent in the model
    pub model_time: Duration,
    /// Tokens per second (completion)
    pub tokens_per_second: f32,
}

/// Streaming token response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StreamToken {
    /// Text for this token
    pub text: String,
    /// Is this the final token
    pub finish_reason: Option<String>,
}

/// Inference request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    /// Prompt for the model
    pub prompt: String,
    /// System message
    pub system: Option<String>,
    /// Configuration for inference
    #[serde(default)]
    pub config: InferenceConfig,
    /// Model to use
    pub model: String,
}

/// Handles inference operations for Deepseek models
pub struct InferenceManager {
    /// API endpoint for model inference
    endpoint: String,
    /// HTTP client
    client: reqwest::Client,
    /// Default model name
    default_model: String,
    /// Active inference requests
    active_requests: Arc<RwLock<HashMap<String, InferenceState>>>,
}

/// State of an inference request
#[derive(Debug)]
struct InferenceState {
    /// When the request started
    start_time: Instant,
    /// Received tokens
    tokens: Vec<Token>,
    /// Token count
    token_count: usize,
    /// Whether the request is streaming
    streaming: bool,
}

impl InferenceManager {
    /// Create a new inference manager
    pub fn new(endpoint: String, default_model: String) -> Self {
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(300))
            .build()
            .expect("Failed to create HTTP client");
            
        Self {
            endpoint,
            client,
            default_model,
            active_requests: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// Get the default model
    pub fn default_model(&self) -> &str {
        &self.default_model
    }
    
    /// Validate an inference configuration
    pub fn validate_config(&self, config: &InferenceConfig) -> Result<()> {
        if config.temperature < 0.0 || config.temperature > 2.0 {
            return Err(DeepseekError::InvalidRequest(
                "Temperature must be between 0.0 and 2.0".to_string()));
        }
        
        if config.top_p < 0.0 || config.top_p > 1.0 {
            return Err(DeepseekError::InvalidRequest(
                "Top-p must be between 0.0 and 1.0".to_string()));
        }
        
        if config.presence_penalty < -2.0 || config.presence_penalty > 2.0 {
            return Err(DeepseekError::InvalidRequest(
                "Presence penalty must be between -2.0 and 2.0".to_string()));
        }
        
        if config.frequency_penalty < -2.0 || config.frequency_penalty > 2.0 {
            return Err(DeepseekError::InvalidRequest(
                "Frequency penalty must be between -2.0 and 2.0".to_string()));
        }
        
        Ok(())
    }
    
    /// Run inference on a prompt
    #[instrument(skip(self, request), fields(model = %request.model, system_len = request.system.as_ref().map(|s| s.len())))]
    pub async fn generate(&self, request: InferenceRequest) -> Result<InferenceResponse> {
        let request_id = uuid::Uuid::new_v4().to_string();
        let start_time = Instant::now();
        
        self.validate_config(&request.config)?;
        
        let model = if request.model.is_empty() {
            self.default_model.clone()
        } else {
            request.model.clone()
        };
        
        // Prepare request payload
        let payload = serde_json::json!({
            "model": model,
            "prompt": request.prompt,
            "system": request.system,
            "temperature": request.config.temperature,
            "top_p": request.config.top_p,
            "top_k": request.config.top_k,
            "max_tokens": request.config.max_tokens,
            "stop": request.config.stop,
            "stream": false,
            "presence_penalty": request.config.presence_penalty,
            "frequency_penalty": request.config.frequency_penalty,
            "seed": request.config.seed,
        });
        
        // Register active request
        {
            let mut active = self.active_requests.write().await;
            active.insert(request_id.clone(), InferenceState {
                start_time,
                tokens: Vec::new(),
                token_count: 0,
                streaming: false,
            });
        }
        
        // Send request
        let response = self.client.post(&self.endpoint)
            .json(&payload)
            .send()
            .await
            .map_err(|e| DeepseekError::NetworkError(format!("Failed to send request: {}", e)))?;
            
        if !response.status().is_success() {
            let error = response.text().await
                .unwrap_or_else(|_| "Unknown error".to_string());
                
            return Err(DeepseekError::ApiError(format!(
                "API request failed with status {}: {}", 
                response.status(), error)));
        }
        
        let api_response: serde_json::Value = response.json().await
            .map_err(|e| DeepseekError::ParseError(format!("Failed to parse response: {}", e)))?;
            
        let completion = api_response["text"].as_str()
            .ok_or_else(|| DeepseekError::ParseError("Missing text in response".to_string()))?
            .to_string();
            
        // Calculate token counts
        let prompt_tokens = api_response["usage"]["prompt_tokens"].as_u64().unwrap_or(0) as u32;
        let completion_tokens = api_response["usage"]["completion_tokens"].as_u64().unwrap_or(0) as u32;
        let total_tokens = api_response["usage"]["total_tokens"].as_u64().unwrap_or(0) as u32;
        
        let elapsed = start_time.elapsed();
        let tokens_per_second = if elapsed.as_secs_f32() > 0.0 && completion_tokens > 0 {
            completion_tokens as f32 / elapsed.as_secs_f32()
        } else {
            0.0
        };
        
        // Remove from active requests
        {
            let mut active = self.active_requests.write().await;
            active.remove(&request_id);
        }
        
        // Build response
        let result = InferenceResponse {
            text: completion,
            tokens: Vec::new(), // We don't have token-level info in non-streaming mode
            usage: TokenUsage {
                prompt_tokens,
                completion_tokens,
                total_tokens,
            },
            metrics: InferenceMetrics {
                total_time: elapsed,
                model_time: elapsed, // We don't have separate model time
                tokens_per_second,
            },
            model,
        };
        
        Ok(result)
    }
    
    /// Generate streaming response
    pub async fn generate_streaming(
        &self, 
        request: InferenceRequest
    ) -> Result<impl Stream<Item = Result<StreamToken>>> {
        let request_id = uuid::Uuid::new_v4().to_string();
        let start_time = Instant::now();
        
        self.validate_config(&request.config)?;
        
        let model = if request.model.is_empty() {
            self.default_model.clone()
        } else {
            request.model.clone()
        };
        
        // Prepare request payload
        let payload = serde_json::json!({
            "model": model,
            "prompt": request.prompt,
            "system": request.system,
            "temperature": request.config.temperature,
            "top_p": request.config.top_p,
            "top_k": request.config.top_k,
            "max_tokens": request.config.max_tokens,
            "stop": request.config.stop,
            "stream": true,
            "presence_penalty": request.config.presence_penalty,
            "frequency_penalty": request.config.frequency_penalty,
            "seed": request.config.seed,
        });
        
        // Register active request
        {
            let mut active = self.active_requests.write().await;
            active.insert(request_id.clone(), InferenceState {
                start_time,
                tokens: Vec::new(),
                token_count: 0,
                streaming: true,
            });
        }
        
        // Set up channel for streaming
        let (tx, rx) = mpsc::channel(100);
        let request_id_clone = request_id.clone();
        let active_requests = self.active_requests.clone();
        
        // Execute stream request
        tokio::spawn(async move {
            let result = async {
                let response = self.client.post(&self.endpoint)
                    .json(&payload)
                    .send()
                    .await
                    .map_err(|e| DeepseekError::NetworkError(format!("Failed to send request: {}", e)))?;
                    
                if !response.status().is_success() {
                    let error = response.text().await
                        .unwrap_or_else(|_| "Unknown error".to_string());
                        
                    return Err(DeepseekError::ApiError(format!(
                        "API request failed with status {}: {}", 
                        response.status(), error)));
                }
                
                let mut stream = response.bytes_stream();
                let mut buffer = String::new();
                
                while let Some(chunk_result) = stream.next().await {
                    let chunk = chunk_result
                        .map_err(|e| DeepseekError::NetworkError(format!("Stream error: {}", e)))?;
                        
                    buffer.push_str(&String::from_utf8_lossy(&chunk));
                    
                    // Process SSE data
                    for line in buffer.lines() {
                        if line.starts_with("data: ") {
                            let data = &line["data: ".len()..];
                            
                            if data == "[DONE]" {
                                // Stream complete
                                let _ = tx.send(Ok(StreamToken {
                                    text: String::new(),
                                    finish_reason: Some("stop".to_string()),
                                })).await;
                                break;
                            }
                            
                            // Parse JSON
                            if let Ok(json) = serde_json::from_str::<serde_json::Value>(data) {
                                if let Some(text) = json["text"].as_str() {
                                    // Update token count
                                    if let Some(mut state) = active_requests.write().await.get_mut(&request_id_clone) {
                                        state.token_count += 1;
                                    }
                                    
                                    let token = StreamToken {
                                        text: text.to_string(),
                                        finish_reason: None,
                                    };
                                    
                                    if tx.send(Ok(token)).await.is_err() {
                                        // Receiver dropped, stop streaming
                                        break;
                                    }
                                }
                            }
                        }
                    }
                    
                    // Keep only the last incomplete line
                    if let Some(last_newline) = buffer.rfind('\n') {
                        buffer = buffer[last_newline + 1..].to_string();
                    }
                }
                
                Ok(())
            }.await;
            
            // Clean up on completion
            {
                let mut active = active_requests.write().await;
                active.remove(&request_id_clone);
            }
            
            if let Err(e) = result {
                let _ = tx.send(Err(e)).await;
            }
        });
        
        // Convert receiver to stream
        let stream = tokio_stream::wrappers::ReceiverStream::new(rx);
        Ok(stream)
    }
    
    /// Count tokens in a text
    pub async fn count_tokens(&self, text: &str, model: Option<&str>) -> Result<usize> {
        // In a real implementation, this would call the tokenizer
        // For now we'll use a simple approximation
        Ok(text.len() / 4) // Very rough approximation
    }
    
    /// Get all active requests
    pub async fn active_requests(&self) -> Vec<String> {
        let active = self.active_requests.read().await;
        active.keys().cloned().collect()
    }
    
    /// Cancel a request
    pub async fn cancel_request(&self, request_id: &str) -> Result<bool> {
        let mut active = self.active_requests.write().await;
        Ok(active.remove(request_id).is_some())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_inference_config_defaults() {
        let config = InferenceConfig::default();
        assert_eq!(config.temperature, 0.7);
        assert_eq!(config.top_p, 0.9);
        assert_eq!(config.top_k, 40);
        assert_eq!(config.max_tokens, 1024);
        assert!(config.stop.is_empty());
        assert!(!config.stream);
    }
    
    #[test]
    fn test_validation() {
        let manager = InferenceManager::new(
            "http://localhost:8000/v1/completions".to_string(),
            "deepseek-coder-6.7b-base".to_string(),
        );
        
        // Valid config
        let config = InferenceConfig::default();
        assert!(manager.validate_config(&config).is_ok());
        
        // Invalid temperature
        let mut invalid_config = InferenceConfig::default();
        invalid_config.temperature = 3.0;
        assert!(manager.validate_config(&invalid_config).is_err());
        
        // Invalid top_p
        let mut invalid_config = InferenceConfig::default();
        invalid_config.top_p = 1.5;
        assert!(manager.validate_config(&invalid_config).is_err());
    }
    
    // Note: Integration tests would go here, but they require
    // a running API server. We'll skip them for now.
} 